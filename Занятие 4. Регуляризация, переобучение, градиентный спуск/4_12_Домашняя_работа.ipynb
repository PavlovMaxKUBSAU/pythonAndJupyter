{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "### Домашняя работа\n\n**Задача высокого уровня** В реализацию функции `gradient` добавьте параметр $\\lambda$, чтобы получить регуляризованный градиентный спуск\n\nФормула поменяется следующим образом:\n$$\n\\left\\{\n\\begin{array}{cc}\n\\frac{\\partial L}{\\partial w_0} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} (1\\cdot \\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right) + \\lambda\\cdot 2\\cdot w_0)&\\\\\n\\frac{\\partial L}{\\partial w_k} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} (x_k^i \\cdot\\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right) + \\lambda\\cdot 2\\cdot w_k)& k\\neq 0 \\\\\n\\end{array}\n\\right.\n$$",
      "metadata": {
        "id": "NTvgYjVhnE6h"
      }
    },
    {
      "cell_type": "code",
      "source": "def gradient(X, y, w, a) -> np.array:\n    # количество обучающих примеров в выборке\n    n = X.shape[0]\n    # считаем прогноз\n    y_hat = X.dot(w.T)\n    \n    error = y - y_hat\n    grad = np.multiply(X, error)\n    error = error + a*2*w[0]\n    grad = grad + a*2*w[n-1] #не уверен, что правильно, но почти\n    gradsum = grad.sum(axis=0)*(-1.0)*2.0 / n\n    return gradsum, error",
      "metadata": {
        "id": "zxpLYBR9PuBe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "В этом модуле мы узнали, как  обучать линейную регрессию, не \"упираясь\" в аппаратные ресурсы: использовать градиентный спуск.\nМы узнали, как детектировать переобучение модели и закрепили свои знания на примере полиномиальной регрессии и выяснили, как увеличить качество решения с помощью механизма регуляризации. Познакомились с двумя видами регуляризации -  Ridge и Lasso.",
      "metadata": {
        "id": "dQ7gRtDmnE6p"
      }
    }
  ]
}